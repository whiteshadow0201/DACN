{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import random\n",
    "from queue import PriorityQueue\n",
    "from copy import deepcopy\n",
    "\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Original graph setup\n",
    "G_original = nx.DiGraph()\n",
    "edges = [\n",
    "    (\"Attacker\", \"Pad\", {\"user\": 0.6, \"root\": 0.6}),\n",
    "    (\"Attacker\", \"Web Server\", {\"user\": 0.8, \"root\": 0.6}),\n",
    "    (\"Attacker\", \"Host 1\", {\"user\": 0.6, \"root\": 0.48}),\n",
    "    (\"Pad\", \"Host 1\", {\"user\": 0.6, \"root\": 0.48}),\n",
    "    (\"Pad\", \"Host 2\", {\"user\": 0.32, \"root\": 0.32}),\n",
    "    (\"Pad\", \"Host 3\", {\"user\": 0.32, \"root\": 0.32}),\n",
    "    (\"Pad\", \"Web Server\", {\"user\": 0.8, \"root\": 0.6}),\n",
    "    (\"Host 1\", \"Pad\", {\"user\": 0.6, \"root\": 0.6}),\n",
    "    (\"Host 1\", \"Web Server\", {\"user\": 0.8, \"root\": 0.6}),\n",
    "    (\"Host 1\", \"Host 2\", {\"user\": 0.32, \"root\": 0.32}),\n",
    "    (\"Host 1\", \"Host 3\", {\"user\": 0.32, \"root\": 0.32}),\n",
    "    (\"Host 2\", \"Host 3\", {\"user\": 0.8, \"root\": 0.8}),\n",
    "    (\"Host 2\", \"File Server\", {\"user\": 0.8, \"root\": 0.6}),\n",
    "    (\"Host 2\", \"Data Server\", {\"user\": 0.8, \"root\": 0.6}),\n",
    "    (\"Host 3\", \"Host 2\", {\"user\": 0.8, \"root\": 0.8}),\n",
    "    (\"Host 3\", \"File Server\", {\"user\": 0.8, \"root\": 0.6}),\n",
    "    (\"Host 3\", \"Data Server\", {\"user\": 0.8, \"root\": 0.6}),\n",
    "    (\"Web Server\", \"File Server\", {\"user\": 0.8, \"root\": 0.04}),\n",
    "    (\"Web Server\", \"Data Server\", {\"user\": 0.8, \"root\": 0.04}),\n",
    "    (\"File Server\", \"Data Server\", {\"user\": 0.8, \"root\": 0.04})\n",
    "]\n",
    "G_original.add_edges_from(edges)\n"
   ],
   "id": "a60ffc5d033479e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Draw the graph\n",
    "plt.figure(figsize=(12, 13))\n",
    "pos = nx.spring_layout(G_original)\n",
    "nx.draw(G_original, pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=2000, font_size=10, font_weight='bold')\n",
    "\n",
    "# Draw edge labels with weights\n",
    "edge_labels = {(u, v): f\"user={d['user']}, root={d['root']}\" for u, v, d in G_original.edges(data=True)}\n",
    "nx.draw_networkx_edge_labels(G_original, pos, edge_labels=edge_labels, font_size=12)\n",
    "\n",
    "plt.show()"
   ],
   "id": "4b79226cb400a676",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Attacker's greedy attack with randomizer\n",
    "def global_weighted_random_attack(graph, honeypot_nodes, goal):\n",
    "    captured = {\"Attacker\"}\n",
    "    path = [\"Attacker\"]\n",
    "\n",
    "    while True:\n",
    "        # Collect all uncaptured neighbors of compromised nodes\n",
    "        neighbors = []\n",
    "        edge_weights = []\n",
    "        source_nodes = []\n",
    "\n",
    "        for compromised_node in captured:\n",
    "            for neighbor in graph.successors(compromised_node):\n",
    "                if neighbor not in captured:\n",
    "                    edge_data = graph[compromised_node][neighbor]\n",
    "                    weight = edge_data['user'] + edge_data['root']\n",
    "                    neighbors.append(neighbor)\n",
    "                    edge_weights.append(weight)\n",
    "                    source_nodes.append(compromised_node)\n",
    "\n",
    "        if not neighbors:\n",
    "            break\n",
    "\n",
    "        # Normalize weights to probabilities\n",
    "        total_weight = sum(edge_weights)\n",
    "        if total_weight == 0:\n",
    "            break\n",
    "        probabilities = [w / total_weight for w in edge_weights]\n",
    "\n",
    "        # Choose next node randomly based on probabilities\n",
    "        chosen_idx = random.choices(range(len(neighbors)), weights=probabilities, k=1)[0]\n",
    "        chosen_node = neighbors[chosen_idx]\n",
    "        source_node = source_nodes[chosen_idx]\n",
    "\n",
    "        # Add to path and captured\n",
    "        path.append(chosen_node)\n",
    "        captured.add(chosen_node)\n",
    "\n",
    "        # Check stopping conditions\n",
    "        if chosen_node in honeypot_nodes or chosen_node == goal:\n",
    "            break\n",
    "\n",
    "    return path, captured\n",
    "\n",
    "# Attacker's greedy attack with randomizer\n",
    "def greedy_attack_priority_queue(graph, honeypot_nodes, goal):\n",
    "    captured = {\"Attacker\"}\n",
    "    path = [\"Attacker\"]\n",
    "    pq = PriorityQueue()\n",
    "    for neighbor in graph.successors(\"Attacker\"):\n",
    "        weight = max(graph[\"Attacker\"][neighbor]['user'], graph[\"Attacker\"][neighbor]['root'])\n",
    "        randomizer = random.uniform(0, 1)  # Randomizer for tie-breaking\n",
    "        pq.put((-weight, -randomizer, neighbor))  # Sort by -weight, -randomizer, neighbor\n",
    "\n",
    "    while not pq.empty():\n",
    "        neg_weight, neg_randomizer, to_node = pq.get()\n",
    "        weight = -neg_weight\n",
    "        randomizer = -neg_randomizer\n",
    "        if to_node in honeypot_nodes:  # Stop at honeypot node\n",
    "            path.append(to_node)\n",
    "            captured.add(to_node)\n",
    "            break\n",
    "        if to_node not in captured:\n",
    "            captured.add(to_node)\n",
    "            path.append(to_node)\n",
    "            if to_node == goal:\n",
    "                break\n",
    "            for next_node in graph.successors(to_node):\n",
    "                if next_node not in captured:\n",
    "                    next_weight = max(graph[to_node][next_node]['user'], graph[to_node][next_node]['root'])\n",
    "                    next_randomizer = random.uniform(0, 1)  # New randomizer for each edge\n",
    "                    pq.put((-next_weight, -next_randomizer, next_node))\n",
    "    return path, captured\n"
   ],
   "id": "9bdc5ea7305d1679",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Environment Setup\n",
    "# Environment class\n",
    "class NetworkSecurityEnv:\n",
    "    def __init__(self, G_original, attack_fn, goal=\"Data Server\"):\n",
    "        self.G_original = G_original\n",
    "        self.attack_fn = attack_fn\n",
    "        self.goal = goal\n",
    "        # Nodes excluding \"Attacker\" and \"Data Server\"\n",
    "        self.nodes = [n for n in G_original.nodes if n not in [\"Attacker\", goal]]\n",
    "        self.num_nodes = len(self.nodes)\n",
    "        self.state = np.zeros(self.num_nodes, dtype=np.float32)  # Initial state: no nodes attacked\n",
    "        self.node_to_idx = {node: idx for idx, node in enumerate(self.nodes)}\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset state to all zeros for a new episode\n",
    "        self.state = np.zeros(self.num_nodes, dtype=np.float32)\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        # Action is a (2, num_nodes) array, each row is a one-hot vector\n",
    "        honeypot_nodes = []\n",
    "        G = deepcopy(self.G_original)\n",
    "\n",
    "        # Process action to place honeypots\n",
    "        for i in range(2):  # Two honeypots\n",
    "            node_idx = np.argmax(action[i])\n",
    "            if action[i, node_idx] == 0:  # Ensure a valid node is selected\n",
    "                node_idx = random.randint(0, self.num_nodes - 1)\n",
    "            node = self.nodes[node_idx]\n",
    "            honeypot = f\"Honeypot_{i}\"\n",
    "            honeypot_nodes.append(honeypot)\n",
    "            # Add honeypot node with edge to selected node\n",
    "            G.add_node(honeypot)\n",
    "            G.add_edge(node, honeypot, user=0.8, root=0.8)\n",
    "\n",
    "        # Simulate attack\n",
    "        path, captured = self.attack_fn(G, honeypot_nodes, self.goal)\n",
    "\n",
    "        # Determine reward and update state\n",
    "        new_state = np.zeros(self.num_nodes, dtype=np.float32)\n",
    "        reward = 0\n",
    "        done = False\n",
    "\n",
    "        if any(h in captured for h in honeypot_nodes):\n",
    "            reward = 1  # Attacker hit a honeypot\n",
    "            done = True\n",
    "        elif self.goal in captured:\n",
    "            reward = -1  # Attacker reached Data Server\n",
    "            done = True\n",
    "\n",
    "        # Update state: mark attacked nodes as 1\n",
    "        for node in captured:\n",
    "            if node in self.node_to_idx:\n",
    "                new_state[self.node_to_idx[node]] = 1\n",
    "\n",
    "        self.state = new_state\n",
    "        return new_state, reward, done, path, captured\n",
    "\n",
    "    def get_action_space_size(self):\n",
    "        # Number of valid configurations: choose 2 different nodes for 2 honeypots\n",
    "        return self.num_nodes * (self.num_nodes - 1)"
   ],
   "id": "9b142c63394d2708",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. DQN Model\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_space_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, action_space_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# Helper to convert action index to 2D array\n",
    "def index_to_action(index, num_nodes):\n",
    "    # Map index to two distinct node indices\n",
    "    first = index // (num_nodes - 1)\n",
    "    second = index % (num_nodes - 1)\n",
    "    if second >= first:\n",
    "        second += 1\n",
    "    action = np.zeros((2, num_nodes), dtype=np.float32)\n",
    "    action[0, first] = 1\n",
    "    action[1, second] = 1\n",
    "    return action\n",
    "\n",
    "def action_to_index(action, num_nodes):\n",
    "    # Convert 2D action array to index\n",
    "    first = np.argmax(action[0])\n",
    "    second = np.argmax(action[1])\n",
    "    if second >= first:\n",
    "        second -= 1\n",
    "    return first * (num_nodes - 1) + second"
   ],
   "id": "142c5c10b61cd2bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return (\n",
    "            np.array(state),\n",
    "            np.array(action),\n",
    "            np.array(reward),\n",
    "            np.array(next_state),\n",
    "            np.array(done)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ],
   "id": "6230132557671d22",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Hàm lưu model (đã có từ trước)\n",
    "def save_model(policy_net, target_net, optimizer, episode, path='dqn_model.pth'):\n",
    "    checkpoint = {\n",
    "        'policy_net_state_dict': policy_net.state_dict(),\n",
    "        'target_net_state_dict': target_net.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'episode': episode\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "    print(f'Model saved to {path}')"
   ],
   "id": "5a10c50f9362259d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Training Loop\n",
    "def train_dqn(env, num_episodes, batch_size=10, gamma=0.99, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995):\n",
    "    global best_checkpoint, best_episode\n",
    "    state_size = env.num_nodes\n",
    "    action_space_size = env.get_action_space_size()\n",
    "\n",
    "    # Initialize DQN and target network\n",
    "    policy_net = DQN(state_size, action_space_size)\n",
    "    target_net = DQN(state_size, action_space_size)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=0.001)\n",
    "    replay_buffer = ReplayBuffer(capacity=10000)\n",
    "    epsilon = epsilon_start\n",
    "    total_reward = 0\n",
    "    dsp = 0\n",
    "    best_dsp = 0\n",
    "    interval_check = num_episodes // 10  # Mỗi num_episodes/10\n",
    "    interval_save = num_episodes // 5   # Lưu sau mỗi num_episodes/5\n",
    "\n",
    "    for episode in range(1,num_episodes+1):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Epsilon-greedy action selection\n",
    "            if random.random() < epsilon:\n",
    "                action_idx = random.randint(0, action_space_size - 1)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "                    q_values = policy_net(state_tensor)\n",
    "                    action_idx = q_values.argmax().item()\n",
    "\n",
    "            action = index_to_action(action_idx, env.num_nodes)\n",
    "            next_state, reward, done, path, captured = env.step(action)\n",
    "            action_idx = action_to_index(action, env.num_nodes)\n",
    "            \n",
    "            # Store experience\n",
    "            replay_buffer.push(state, action_idx, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            if reward == 1 :\n",
    "                dsp += 1\n",
    "            # Train if enough experiences\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "\n",
    "                states = torch.FloatTensor(states)\n",
    "                actions = torch.LongTensor(actions)\n",
    "                rewards = torch.FloatTensor(rewards)\n",
    "                next_states = torch.FloatTensor(next_states)\n",
    "                dones = torch.FloatTensor(dones)\n",
    "\n",
    "                # Compute Q-values\n",
    "                q_values = policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "                # Compute target Q-values\n",
    "                with torch.no_grad():\n",
    "                    next_q_values = target_net(next_states).max(1)[0]\n",
    "                    targets = rewards + (1 - dones) * gamma * next_q_values\n",
    "\n",
    "                # Compute loss\n",
    "                loss = nn.MSELoss()(q_values, targets)\n",
    "\n",
    "                # Optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Update target network\n",
    "        if episode % 10 == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        # Decay epsilon\n",
    "        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "\n",
    "        # Logging\n",
    "        if episode  % interval_check == 0:\n",
    "            placement = []\n",
    "            for i in range(2):  # Two honeypots\n",
    "                node_idx = np.argmax(action[i])\n",
    "                node_name = env.nodes[node_idx]\n",
    "                placement.append(f\"Honeypot {i} -> {node_name}\\n\")\n",
    "            print(f\"Episode {episode}, Total Reward: {total_reward}, Epsilon: {epsilon:.3f}, Defense Success Probability: {dsp/interval_check}%\\n\")\n",
    "            print(\"\".join(placement))\n",
    "            print(path)\n",
    "            total_reward = 0\n",
    "            \n",
    "            # Log ra DSP lớn nhất sau mỗi num_episodes/10 iterations\n",
    "            if dsp > best_dsp:\n",
    "                best_dsp = dsp\n",
    "                best_episode = episode\n",
    "                best_checkpoint = {\n",
    "                    'policy_net_state_dict': deepcopy(policy_net.state_dict()),\n",
    "                    'target_net_state_dict': deepcopy(target_net.state_dict()),\n",
    "                    'optimizer_state_dict': deepcopy(optimizer.state_dict()),\n",
    "                    'episode': episode\n",
    "                }\n",
    "            # Reset DSP\n",
    "            dsp = 0\n",
    "\n",
    "                \n",
    "        # Save ra DSP lớn nhất sau mỗi num_episodes/5 iterations\n",
    "        if (episode + 1) % interval_save == 0 and best_checkpoint is not None:\n",
    "            path = f'./Saved_Model/dqn_model.pth'\n",
    "            torch.save({\n",
    "                'policy_net_state_dict': best_checkpoint['policy_net_state_dict'],\n",
    "                'target_net_state_dict': best_checkpoint['target_net_state_dict'],\n",
    "                'optimizer_state_dict': best_checkpoint['optimizer_state_dict'],\n",
    "                'episode': best_episode},\n",
    "                path)\n",
    "            print(f'Saved model with best DSP {best_dsp} at episode {best_episode} to {path}')\n",
    "            \n",
    "            best_dsp = 0\n",
    "            best_episode = 0\n",
    "            best_checkpoint = None\n",
    "            \n",
    "            \n",
    "    return policy_net\n",
    "\n",
    "# Initialize environment and train\n",
    "algo = global_weighted_random_attack\n",
    "# algo = greedy_attack_priority_queue\n",
    "env = NetworkSecurityEnv(G_original, algo)\n",
    "num_episode = 10000\n",
    "policy_net = train_dqn(env, num_episode)"
   ],
   "id": "5377d66c524211df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def evaluate_model(model, env, num_episodes=1000):\n",
    "    successes = 0\n",
    "    for episode in range(1,num_episodes+1):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        episode_honeypots = []  # Lưu vị trí honeypot trong episode\n",
    "        \n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "                q_values = model(state_tensor)\n",
    "                action_idx = q_values.argmax().item()\n",
    "                action = index_to_action(action_idx, env.num_nodes)\n",
    "            next_state, reward, done, path, captured = env.step(action)\n",
    "      \n",
    "            state = next_state\n",
    "            honeypot_nodes = []\n",
    "            for i in range(2):\n",
    "                node_idx = np.argmax(action[i])\n",
    "                honeypot_nodes.append(env.nodes[node_idx])\n",
    "            print(\"Episode:\",episode )\n",
    "            if reward == 1:  # Honeypot bẫy được kẻ tấn công\n",
    "                successes += 1\n",
    "                print (path)\n",
    "                print(f\"Success\\nHoneypots: {action}\\nHoneypots connected to: {honeypot_nodes}\\n\")\n",
    "                break\n",
    "            elif reward == -1:  # Kẻ tấn công đạt mục tiêu\n",
    "                print (path)\n",
    "                print(f\"Failed\\nHoneypots: {action}\\nHoneypots connected to: {honeypot_nodes}\\n\")\n",
    "                break\n",
    "        \n",
    "        \n",
    "    dsp = (successes / num_episodes ) * 100\n",
    "    print(f\"\\nDefense success probability: {dsp:.2f}%\")\n"
   ],
   "id": "33399fff2a66d38e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "evaluate_model(policy_net,env)",
   "id": "597785563e2df568",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
